{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepRhythmModel(\n",
      "  (conv1): Conv2d(6, 128, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(128, 64, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 32, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
      "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(32, 8, kernel_size=(120, 6), stride=(1, 1))\n",
      "  (bn5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=2904, out_features=256, bias=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepRhythmModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepRhythmModel, self).__init__()\n",
    "\n",
    "        # Assuming the input shape is (240, 8, 6) which is (φ, b, h)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Updated to reflect the simplified flattening size of 5760\n",
    "        self.conv1 = nn.Conv2d(in_channels=6, out_channels=128, kernel_size=(4, 6), padding='same')\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(4, 6), padding='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(4, 6), padding='same')\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(4, 6), padding='same')\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # The kernel size for the last convolutional layer covers the entire φ dimension\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(120, 6))\n",
    "        self.bn5 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # The output of the last convolutional layer is (1, 1, 8), so after flattening it becomes 8*1*1 = 8\n",
    "        self.fc1 = nn.Linear(2904, 256)  # The flattening size is now 8*1*6 = 48\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        x = self.dropout(self.elu(self.fc1(x)))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Number of classes based on the problem specification\n",
    "num_classes = 256\n",
    "\n",
    "# Instantiate the simplified model\n",
    "simplified_model = DeepRhythmModel(num_classes)\n",
    "\n",
    "# Check simplified model architecture\n",
    "print(simplified_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy output shape: torch.Size([1, 256])\n",
      "Model setup is valid and forward pass works.\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 6, 240, 8)  # batch size of 1 for testing\n",
    "\n",
    "# Move the model to an appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = simplified_model.to(device)\n",
    "dummy_input = dummy_input.to(device)\n",
    "\n",
    "# Perform a forward pass with the dummy input\n",
    "try:\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(\"Dummy output shape:\", dummy_output.shape)\n",
    "    print(\"Model setup is valid and forward pass works.\")\n",
    "except Exception as e:\n",
    "    print(\"Model setup has issues:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "def bpm_to_class(bpm, min_bpm=30, max_bpm=286, num_classes=256):\n",
    "    \"\"\"Map a BPM value to a class index.\"\"\"\n",
    "    # Linearly map BPM values to class indices\n",
    "    class_width = (max_bpm - min_bpm) / num_classes\n",
    "    class_index = int((bpm - min_bpm) // class_width)\n",
    "    return max(0, min(num_classes - 1, class_index))\n",
    "\n",
    "def class_to_bpm(class_index, min_bpm=30, max_bpm=286, num_classes=256):\n",
    "    \"\"\"Map a class index back to a BPM value (to the center of the class interval).\"\"\"\n",
    "    class_width = (max_bpm - min_bpm) / num_classes\n",
    "    bpm = min_bpm + class_width * (class_index + 0.5)\n",
    "    return bpm\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file, transform=None):\n",
    "        self.file_path = hdf5_file\n",
    "        self.transform = transform\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.items = []\n",
    "            for group_name in file.keys():\n",
    "                group = file[group_name]\n",
    "                for item_name in group.keys():\n",
    "                    self.items.append((group_name, item_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group_name, item_name = self.items[idx]\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            item = file[group_name][item_name]\n",
    "            data = torch.tensor(item['hcqm'][:], dtype=torch.float)\n",
    "            bpm = torch.tensor([item.attrs['bpm']], dtype=torch.int)\n",
    "        label_class_index = bpm_to_class(bpm)  # Convert BPM to class index\n",
    "        data = data.permute(2, 0, 1)\n",
    "        return data, label_class_index\n",
    "\n",
    "def split_dataset(dataset, train_ratio, test_ratio, validate_ratio):\n",
    "    total_ratio = train_ratio + test_ratio + validate_ratio\n",
    "    assert abs(total_ratio - 1) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    test_size = int(test_ratio * dataset_size)\n",
    "    validate_size = dataset_size - train_size - test_size\n",
    "\n",
    "    train_dataset, test_dataset, validate_dataset = random_split(dataset, [train_size, test_size, validate_size])\n",
    "    return train_dataset, test_dataset, validate_dataset\n",
    "\n",
    "# Example usage\n",
    "dataset = HDF5Dataset('output_data2.hdf5')\n",
    "# train_dataset, test_dataset, validate_dataset = split_dataset(dataset, 0.7, 0.2, 0.1)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "# validate_loader = DataLoader(validate_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Now you have DataLoaders for training, testing, and validation.\n",
    "# len(train_dataset.indices), len(test_dataset.indices), len(validate_dataset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: torch.Size([32, 240, 8, 6])\n",
      "Labels batch shape: torch.Size([32, 1])\n",
      "First few labels: tensor([[170],\n",
      "        [128],\n",
      "        [189],\n",
      "        [ 74],\n",
      "        [157]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46979"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_split_indices(train_dataset, test_dataset, validate_dataset, filename=\"dataset_splits.json\"):\n",
    "    # Extract indices from the subsets\n",
    "    splits = {\n",
    "        'train_indices': train_dataset.indices,\n",
    "        'test_indices': test_dataset.indices,\n",
    "        'validate_indices': validate_dataset.indices\n",
    "    }\n",
    "    # Save to JSON file\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(splits, f)\n",
    "\n",
    "# Assuming you've already created train_dataset, test_dataset, and validate_dataset\n",
    "save_split_indices(train_dataset, test_dataset, validate_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import json\n",
    "def load_split_datasets(dataset, filename=\"dataset_splits.json\"):\n",
    "    # Load the saved indices\n",
    "    with open(filename, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "\n",
    "    # Recreate the subsets using the loaded indices\n",
    "    train_dataset = Subset(dataset, splits['train_indices'])\n",
    "    test_dataset = Subset(dataset, splits['test_indices'])\n",
    "    validate_dataset = Subset(dataset, splits['validate_indices'])\n",
    "\n",
    "    return train_dataset, test_dataset, validate_dataset\n",
    "\n",
    "# Assuming dataset is your full HDF5Dataset instance\n",
    "train_dataset, test_dataset, validate_dataset = load_split_datasets(dataset)\n",
    "\n",
    "# Now you can create DataLoader instances as before\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32885, 9395, 4699)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.indices), len(test_dataset.indices), len(validate_dataset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: torch.Size([32, 240, 8, 6])\n",
      "Labels batch shape: torch.Size([32])\n",
      "First few labels: tensor([ 60, 102,  92, 174,  82])\n"
     ]
    }
   ],
   "source": [
    "# Load a single batch from the dataloader\n",
    "data_iter = iter(train_loader)\n",
    "data, labels = next(data_iter)\n",
    "\n",
    "# Print the shapes of the data and labels to verify\n",
    "print(f\"Data batch shape: {data.shape}\")  # Should be [batch_size, 240, 8, 6]\n",
    "print(f\"Labels batch shape: {labels.shape}\")  # Shape depends on how you've set up labels\n",
    "\n",
    "# Print the first few labels to check they're loaded correctly\n",
    "print(f\"First few labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.6976, Validate Loss: 2.9462\n",
      "Epoch 2, Train Loss: 3.0090, Validate Loss: 2.8088\n",
      "Epoch 3, Train Loss: 2.7967, Validate Loss: 2.7384\n",
      "Epoch 4, Train Loss: 2.6433, Validate Loss: 2.6563\n",
      "Epoch 5, Train Loss: 2.5019, Validate Loss: 2.6279\n",
      "Epoch 6, Train Loss: 2.3472, Validate Loss: 2.5916\n",
      "Epoch 7, Train Loss: 2.1786, Validate Loss: 2.6046\n",
      "Epoch 8, Train Loss: 2.0020, Validate Loss: 2.6053\n",
      "Epoch 9, Train Loss: 1.8201, Validate Loss: 2.6179\n",
      "Epoch 10, Train Loss: 1.6141, Validate Loss: 2.6340\n",
      "Epoch 11, Train Loss: 1.4168, Validate Loss: 2.7615\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'HDF5Dataset' is your dataset class and 'model' is your CNN model instance\n",
    "hdf5_file_path='output_data2.hdf5'\n",
    "dataset = HDF5Dataset(hdf5_file_path)\n",
    "train_dataset, test_dataset, validate_dataset = load_split_datasets(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "# Early stopping setup\n",
    "early_stopping_patience = 5\n",
    "early_stopping_counter = 0\n",
    "best_validate_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validate_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validate_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validate_loss += loss.item()\n",
    "\n",
    "    average_train_loss = running_loss / len(train_loader)\n",
    "    average_validate_loss = validate_loss / len(validate_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {average_train_loss:.4f}, Validate Loss: {average_validate_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if average_validate_loss < best_validate_loss:\n",
    "        best_validate_loss = average_validate_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'deeprhythm0.1.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepRhythmModel(\n",
       "  (conv1): Conv2d(6, 128, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 32, kernel_size=(4, 6), stride=(1, 1), padding=same)\n",
       "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(32, 8, kernel_size=(120, 6), stride=(1, 1))\n",
       "  (bn5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2904, out_features=256, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepRhythmModel(256)\n",
    "\n",
    "# Load the weights\n",
    "model.load_state_dict(torch.load('deeprhythm0.1.pth'))\n",
    "model = model.cpu() #.to(device='cuda')\n",
    "# Ensure to call model.eval() to set dropout and batch normalization layers to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def load_and_split_audio(filename, sr=22050, clip_length=8):\n",
    "    \"\"\"\n",
    "    Load an audio file, split it into 8-second clips, and return a single tensor of all clips.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: Path to the audio file.\n",
    "    - sr: Sampling rate to use for loading the audio.\n",
    "    - clip_length: Length of each clip in seconds.\n",
    "\n",
    "    Returns:\n",
    "    A tensor of shape [clips, audio] where each row is an 8-second clip.\n",
    "    \"\"\"\n",
    "\n",
    "    clips = []\n",
    "    clip_samples = sr * clip_length\n",
    "    try:\n",
    "        audio, _ = librosa.load(filename, sr=sr)\n",
    "        for i in range(0, len(audio), clip_samples):\n",
    "            if i + clip_samples <= len(audio):\n",
    "                clip_tensor = torch.tensor(audio[i:i + clip_samples], dtype=torch.float32)\n",
    "                clips.append(clip_tensor)\n",
    "    except Exception as e:\n",
    "        print(e, filename)\n",
    "\n",
    "    # Stack all clips along a new dimension to form a single tensor\n",
    "    if clips:\n",
    "        stacked_clips = torch.stack(clips, dim=0)\n",
    "    else:\n",
    "        # Return an empty tensor if no clips were created (file is shorter than clip_length)\n",
    "        return None\n",
    "\n",
    "    # Share memory of the stacked clips tensor\n",
    "    stacked_clips.share_memory_()\n",
    "\n",
    "    return stacked_clips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT kernels created, time used = 0.0440 seconds\n",
      "CQT kernels created, time used = 0.0263 seconds\n",
      "CQT kernels created, time used = 0.0190 seconds\n",
      "CQT kernels created, time used = 0.0178 seconds\n",
      "CQT kernels created, time used = 0.0143 seconds\n",
      "CQT kernels created, time used = 0.0138 seconds\n",
      "CQT kernels created, time used = 0.0144 seconds\n",
      "torch.Size([24, 6, 240, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bleu/anaconda3/envs/autoawq/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/Convolution.cpp:1003.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "162.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hcqm import make_specs, compute_hcqm\n",
    "\n",
    "def predict_global_bpm(model, input_path):\n",
    "    clips = load_and_split_audio(input_path, sr=22050)\n",
    "    sr=22050\n",
    "    model_device = next(model.parameters()).device\n",
    "    len_audio = sr*8\n",
    "    stft, band, cqt = make_specs(len_audio, sr, device=model_device)\n",
    "    input_batch = compute_hcqm(clips.to(device=model_device), stft, band, cqt).permute(0,3,1,2)\n",
    "    print(input_batch.shape)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Ensure the batch is on the same device as the model\n",
    "        input_batch = input_batch.to(device=model_device)\n",
    "        outputs = model(input_batch)\n",
    "\n",
    "        # Apply softmax to convert logits to probabilities\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "\n",
    "        # Compute the average probability across the batch for each class\n",
    "        mean_probabilities = probabilities.mean(dim=0)\n",
    "\n",
    "        # Find the class with the maximum average probability\n",
    "        _, predicted_class = torch.max(mean_probabilities, 0)\n",
    "        predicted_global_bpm = class_to_bpm(predicted_class.item())\n",
    "\n",
    "    return predicted_global_bpm\n",
    "\n",
    "predict_global_bpm(model,'data/Anime World.flac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'HDF5Dataset' is your dataset class and 'model' is your CNN model instance\n",
    "hdf5_file_path='output_data2.hdf5'\n",
    "dataset = HDF5Dataset(hdf5_file_path)\n",
    "train_dataset, test_dataset, validate_dataset = load_split_datasets(dataset)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_acc1():\n",
    "    model = DeepRhythmModel(256)\n",
    "    # Load the weights\n",
    "    model.load_state_dict(torch.load('deeprhythm0.1.pth'))\n",
    "    model = model.to(device='cuda')\n",
    "    # Ensure to call model.eval() to set dropout and batch normalization layers to evaluation mode\n",
    "    model.eval()\n",
    "    device = 'cuda'\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in test_loader:  # Assuming 'test_loader' is your DataLoader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the same device as model\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            # Convert predicted and true class indices to BPM\n",
    "            predicted_bpms = torch.tensor([class_to_bpm(pred.item()) for pred in predicted], device=device)\n",
    "            true_bpms = torch.tensor([class_to_bpm(label.item()) for label in labels], device=device)\n",
    "\n",
    "            # Calculate the acceptable range based on 4% of the true BPMs\n",
    "            lower_bounds = true_bpms * 0.96  # 4% lower\n",
    "            upper_bounds = true_bpms * 1.04  # 4% higher\n",
    "\n",
    "            # Determine which predictions are correct within the acceptable range\n",
    "            correct_preds = (predicted_bpms >= lower_bounds) & (predicted_bpms <= upper_bounds)\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Acc1 on the test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc2():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted_classes = torch.max(probabilities, dim=1)\n",
    "\n",
    "            # Convert predicted and true class indices to BPM\n",
    "            predicted_bpms = torch.tensor([class_to_bpm(pred.item()) for pred in predicted_classes], device=device)\n",
    "            true_bpms = torch.tensor([class_to_bpm(label.item()) for label in labels], device=device)\n",
    "\n",
    "            # Initialize a tensor to track correctness\n",
    "            correct_preds = torch.zeros_like(predicted_bpms, dtype=torch.bool)\n",
    "\n",
    "            # Check against multiples of the true BPM\n",
    "            for multiple in [0.5, 1, 2]:  # Consider 1/2x, 1x, and 2x the true BPM\n",
    "                adjusted_true_bpms = true_bpms * multiple\n",
    "                lower_bounds = adjusted_true_bpms * 0.96\n",
    "                upper_bounds = adjusted_true_bpms * 1.04\n",
    "\n",
    "                # Update correct predictions\n",
    "                correct_mask = (predicted_bpms >= lower_bounds) & (predicted_bpms <= upper_bounds)\n",
    "                correct_preds = correct_preds | correct_mask\n",
    "\n",
    "            correct += correct_preds.sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Acc2 (considering multiples): {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc1 on the test set: 64.54%\n",
      "Acc2 (considering multiples): 73.34%\n"
     ]
    }
   ],
   "source": [
    "calc_acc1()\n",
    "calc_acc2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoawq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
